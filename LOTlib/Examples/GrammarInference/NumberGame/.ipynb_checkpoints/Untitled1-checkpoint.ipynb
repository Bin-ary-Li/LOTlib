{
 "metadata": {
  "name": "",
  "signature": "sha256:d3841eaf5784660612e338fb73cf56e65c3343d973b204b76d91a90e33f32065"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab qt\n",
      "from LOTlib.Hypotheses.GrammarHypothesis import GrammarHypothesis\n",
      "from LOTlib.Inference.MetropolisHastings import MHSampler\n",
      "from LOTlib.Inference.PriorSample import prior_sample\n",
      "from LOTlib.Examples.NumberGame.NewVersion.Model import *\n",
      "from Model import *\n",
      "from LOTlib.Visualization.SampleCollector import *\n",
      "from matplotlib.widgets import Slider"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parameters for number game inference\n",
      "alpha = 0.99\n",
      "n = 1000\n",
      "domain = 20\n",
      "\n",
      "# Parameters for GrammarHypothesis inference\n",
      "grammar_n = 100000\n",
      "data = toy_2n\n",
      "\n",
      "# Variables for NumberGameHypothesis inference\n",
      "h0 = make_h0(grammar=simple_test_grammar, domain=domain, alpha=alpha)\n",
      "mh_sampler = MHSampler(h0, data[0].input, n)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Generate number game hypotheses'''\n",
      "hypotheses = set()\n",
      "for h in lot_iter(mh_sampler):\n",
      "    hypotheses.add(h)\n",
      "\n",
      "for h in hypotheses:\n",
      "    print h, h(), h.domain, h.alpha"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "lambda : mapset_(lambda y1: times_(y1, 3), range_set_(1, 20, bound=20)) [3, 6, 9, 12, 15, 18] 20 0.99\n",
        "lambda : mapset_(lambda y1: times_(y1, 1), range_set_(1, 20, bound=20)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] 20 0.99\n",
        "lambda : mapset_(lambda y1: times_(y1, 2), range_set_(1, 20, bound=20)) [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 20 0.99\n",
        "lambda : mapset_(lambda y1: times_(y1, 7), range_set_(1, 20, bound=20)) [7, 14] 20 0.99\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''What grammar probabilities will best model our human data?'''\n",
      "grammar_h0 = GrammarHypothesis(simple_test_grammar, hypotheses, proposal_step=.1, proposal_n=1)\n",
      "\n",
      "for r in grammar_h0.rules:\n",
      "    print r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SET -> mapset_['FUNC', 'RANGE'] w/ p=1.0, resample_p=1.0\n",
        "EXPR -> times_['X', '1'] w/ p=1.0, resample_p=1.0\n",
        "EXPR -> times_['X', '2'] w/ p=1.0, resample_p=1.0\n",
        "EXPR -> times_['X', '3'] w/ p=1.0, resample_p=1.0\n",
        "EXPR -> times_['X', '7'] w/ p=1.0, resample_p=1.0\n",
        "START -> ['SET'] w/ p=1.0, resample_p=1.0\n",
        "RANGE -> range_set_['1', '20', 'bound=20'] w/ p=1.0, resample_p=1.0\n",
        "FUNC -> lambda['EXPR'] w/ p=1.0, resample_p=1.0BV:X;None;y\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''GrammarHypothesis inference'''\n",
      "mh_grammar_sampler = MHSampler(grammar_h0, data, grammar_n, trace=False)\n",
      "\n",
      "grammar_hypotheses = []\n",
      "i = 0\n",
      "samplec = VectorSampleCollector(rate=100)\n",
      "for grammar_h in lot_iter(mh_grammar_sampler):\n",
      "    i += 1\n",
      "    samplec.add_sample(grammar_h)\n",
      "    grammar_hypotheses.append(grammar_h)\n",
      "    if i % (mh_grammar_sampler.steps/20) == 0:\n",
      "        print ['%.3f' % v for v in grammar_h.value]\n",
      "        print i, '='*70\n",
      "        print grammar_h.prior, grammar_h.likelihood, grammar_h.posterior_score\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['1.000', '0.089', '1.889', '2.402', '0.528', '1.000', '1.000', '1.000']\n",
        "5000 ======================================================================\n",
        "-10.4513716809 -2.22753167366 -12.6789033545\n",
        "['1.000', '0.103', '3.245', '3.494', '2.012', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10000 ======================================================================\n",
        "-11.99782844 -1.73650323442 -13.7343316744\n",
        "['1.000', '0.134', '2.638', '0.508', '1.874', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15000 ======================================================================\n",
        "-10.2445023839 -2.12239339782 -12.3668957817\n",
        "['1.000', '0.702', '7.586', '2.967', '0.745', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20000 ======================================================================\n",
        "-13.5337644044 -3.43908043623 -16.9728448406\n",
        "['1.000', '0.536', '4.773', '0.239', '2.101', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25000 ======================================================================\n",
        "-11.3987401957 -4.11077792023 -15.509518116\n",
        "['1.000', '0.161', '6.612', '1.214', '3.571', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30000 ======================================================================\n",
        "-14.0312195169 -1.1287539928 -15.1599735097\n",
        "['1.000', '0.394', '4.991', '1.236', '1.364', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35000 ======================================================================\n",
        "-10.7866628554 -2.97620000944 -13.7628628648\n",
        "['1.000', '0.144', '2.527', '2.445', '0.717', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40000 ======================================================================\n",
        "-10.2822392535 -2.46353128171 -12.7457705352\n",
        "['1.000', '0.099', '2.161', '0.420', '2.002', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45000 ======================================================================\n",
        "-10.3979905862 -2.02516895885 -12.4231595451\n",
        "['1.000', '0.729', '5.269', '1.242', '1.656', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50000 ======================================================================\n",
        "-10.8288098831 -5.01848675208 -15.8472966352\n",
        "['1.000', '0.345', '6.821', '2.694', '0.355', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55000 ======================================================================\n",
        "-13.4031712771 -1.96454930391 -15.367720581\n",
        "['1.000', '0.547', '6.062', '3.255', '5.162', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60000 ======================================================================\n",
        "-15.005425877 -3.6535389323 -18.6589648093\n",
        "['1.000', '0.104', '4.094', '1.142', '0.849', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65000 ======================================================================\n",
        "-11.0706855056 -1.08807262822 -12.1587581338\n",
        "['1.000', '0.188', '5.708', '1.191', '0.473', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70000 ======================================================================\n",
        "-12.065401575 -1.28148715986 -13.3468887348\n",
        "['1.000', '0.217', '3.539', '0.981', '0.386', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75000 ======================================================================\n",
        "-10.3584797725 -2.31877881788 -12.6772585904\n",
        "['1.000', '0.204', '2.694', '0.966', '3.281', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80000 ======================================================================\n",
        "-10.5909782 -3.21458068385 -13.8055588839\n",
        "['1.000', '0.278', '10.356', '0.272', '3.162', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85000 ======================================================================\n",
        "-17.1630917661 -1.08119125601 -18.2442830221\n",
        "['1.000', '0.546', '10.520', '1.413', '0.745', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90000 ======================================================================\n",
        "-15.4244520777 -1.92503549725 -17.349487575\n",
        "['1.000', '0.197', '10.628', '1.849', '3.138', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95000 ======================================================================\n",
        "-17.3167648492 -0.832807234747 -18.149572084\n",
        "['1.000', '0.414', '4.060', '1.084', '3.585', '1.000', '1.000', '1.000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100000 ======================================================================\n",
        "-11.2664459216 -3.97324194367 -15.2396878652\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numpy array of sampled values for each vector element altered in proposals\n",
      "vector_data = zip(*[[s.value[i] for i in s.propose_idxs] for s in samplec.samples])\n",
      "vector_data = [np.array(l) for l in vector_data]\n",
      "\n",
      "# Set up initial violinplot\n",
      "fig, ax = plt.subplots()\n",
      "fig.subplots_adjust(bottom=0.2, left=0.1)\n",
      "ax.set_title('Distribution of values over GrammarRules generated by MH')\n",
      "ax.violinplot(vector_data, points=100, vert=False, widths=0.7,\n",
      "              showmeans=True, showextrema=True, showmedians=True)\n",
      "max_interval = ax.xaxis.get_view_interval()\n",
      "\n",
      "def update_violinplot(value):\n",
      "    \"\"\"Update the violinplot when you move the slider\"\"\"\n",
      "    ax.clear()\n",
      "    data = [vector[0:value] for vector in vector_data]\n",
      "    ax.violinplot(data, points=100, vert=False, widths=0.7,\n",
      "                  showmeans=True, showextrema=True, showmedians=True)\n",
      "    ax.xaxis.set_view_interval(max_interval)\n",
      "    fig.canvas.draw_idle()\n",
      "\n",
      "\n",
      "# Add slider to plot; slider updates violinplot as a function of how many samples have been generated\n",
      "slider_ax = plt.axes([0.1, 0.1, 0.8, 0.02])    \n",
      "slider = Slider(slider_ax, \"after N samples\", valmin=1., valmax=samplec.sample_count, valinit=1.)\n",
      "slider.on_changed(update_violinplot)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "scrap\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Print distribution over power rule:  [prior, likelihood, posterior]'''\n",
      "# vals, posteriors = grammar_h0.rule_distribution(data, 'ipowf_', np.arange(0.1, 5., 0.1))\n",
      "# print_dist(vals, posteriors)\n",
      "#visualize_dist(vals, posteriors, 'union_')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "'print distribution over power rule:  [prior, likelihood, posterior]'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%qtconsole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}